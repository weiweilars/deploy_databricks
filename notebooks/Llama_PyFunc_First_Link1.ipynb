{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df378bfe-35bc-4029-9cf4-542e60b82c06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import accelerate\n",
    "\n",
    "from transformers import pipeline\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.transformers import generate_signature_output\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bad49dc3-6fe9-4dec-b0db-582976def633",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# model = pipeline('text-generation', model='meta-llama/Llama-2-7b-chat-hf', device_map = \"auto\",load_in_8bit=True)\n",
    "snapshot_location = os.path.expanduser(\"~/.cache/huggingface/model\")\n",
    "os.makedirs(snapshot_location, exist_ok=True)\n",
    "# model.save_pretrained(snapshot_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24b878c7-6e62-4d1f-8ea2-1967c883d9b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04377f3c66a45909e73304e71ebe96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8bcc19e3984abd8ee43b1412ad9cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)434a8/.gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3464708c48cb46659b3539a4f8721a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fdadf434a8/README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b1097b21704cb699d5951cb44abae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)adf434a8/LICENSE.txt:   0%|          | 0.00/7.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6d9cbdad4c49caa3d5407626fa4fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)f434a8/USE_POLICY.md:   0%|          | 0.00/4.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb649dd14ed3402f8b1cbb7c93860a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36333ef63e6645e1896da8fef34d7e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b3769cfc3d4ea58cf4b0d30e83e93e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)adf434a8/config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658e7922c77b491984fa72d4513f1654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2b3566aece4f13b498a1be466b3414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6cb5bfc7c31422da5816dea51c9b9aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)434a8/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3f026897d148f1b66de561900a9572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f2e8451f4d450eb439824fe5d41f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58b5e90bc09447eac0ac689f2d076ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e426063068409ba04c8fb47e1f383d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09bc7111b1e74776b8172fb982a7a0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a654bbc6c4504efe837182370ebcc0a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "snapshot_location = snapshot_download(repo_id=\"meta-llama/Llama-2-7b-chat-hf\", local_dir=snapshot_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35b3c6fe-e833-47e6-b4ed-39327184a20e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/root/.cache/huggingface/model'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f56cafff-66e2-446b-8211-8f8654808a32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Llama_pyfunc(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        This method initializes the tokenizer and language model\n",
    "        using the specified model snapshot directory.\n",
    "        \"\"\"\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "        import accelerate\n",
    "\n",
    "\n",
    "        # Initialize tokenizer and language model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            context.artifacts[\"snapshot\"], padding_side=\"left\"\n",
    "        )\n",
    "\n",
    "        # config = AutoConfig.from_pretrained(\n",
    "        #     context.artifacts[\"snapshot\"], trust_remote_code=True\n",
    "        # )\n",
    "        # If you are running this in a system that has a sufficiently powerful GPU with available VRAM,\n",
    "        # uncomment the configuration setting below to leverage triton.\n",
    "        # Note that triton dramatically improves the inference speed performance\n",
    "\n",
    "        # config.attn_config[\"attn_impl\"] = \"triton\"\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            context.artifacts[\"snapshot\"],\n",
    "            # torch_dtype=torch.bfloat16,\n",
    "            load_in_4bit=True,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "    def _build_prompt(self, instruction):\n",
    "        \"\"\"\n",
    "        This method generates the prompt for the model.\n",
    "        \"\"\"\n",
    "\n",
    "        return instruction\n",
    "\n",
    "\n",
    "    def predict(self, context, model_input, params=None):\n",
    "        \"\"\"\n",
    "        This method generates prediction for the given input.\n",
    "        \"\"\"\n",
    "        prompt = model_input[\"prompt\"][0]\n",
    "\n",
    "        # Retrieve or use default values for temperature and max_tokens\n",
    "        temperature = params[\"temperature\"] if params else 0.1\n",
    "        max_tokens = params[\"max_tokens\" ]if params else 100\n",
    "\n",
    "        # Build the prompt\n",
    "        prompt = self._build_prompt(prompt)\n",
    "\n",
    "        # Encode the input and generate prediction\n",
    "        # NB: Sending the tokenized inputs to the GPU here explicitly will not work if your system does not have CUDA support.\n",
    "        # If attempting to run this with GPU support, change 'cpu' to 'cuda' for maximum performance\n",
    "        encoded_input = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        output = self.model.generate(\n",
    "            encoded_input,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            max_new_tokens=max_tokens,\n",
    "        )\n",
    "\n",
    "        # Decode the prediction to text\n",
    "        generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        # Removing the prompt from the generated text\n",
    "        prompt_length = len(self.tokenizer.encode(prompt, return_tensors=\"pt\")[0])\n",
    "        generated_response = self.tokenizer.decode(\n",
    "            output[0][prompt_length:], skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        return {\"predictions\": [generated_response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7be4c91d-5d1b-4eda-8b03-89c444c9075c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from mlflow.models.signature import ModelSignature, infer_signature\n",
    "from mlflow.types import DataType, Schema, ColSpec, ParamSchema, ParamSpec\n",
    "\n",
    "# Define input and output schema\n",
    "# input_schema = Schema(\n",
    "#     [\n",
    "#         ColSpec(DataType.string, \"prompt\"),\n",
    "#     ]\n",
    "# )\n",
    "# output_schema = Schema([ColSpec(DataType.string, \"candidates\")])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parameters = {\"temperature\":0.1,\"max_tokens\":150}\n",
    "\n",
    "input_example = pd.DataFrame({\"prompt\": [\"Hello, I'm a language model,\"]})\n",
    "output_example = pd.DataFrame({\"predictions\": [\"How can I help you\"]})\n",
    "# inference_config={\"max_new_tokens\": 50, \"temperature\": 0.1}\n",
    "signature = infer_signature(input_example, output_example, params=parameters)\n",
    "\n",
    "# signature = ModelSignature(inputs=input_schema, outputs=output_schema, params=parameters)\n",
    "\n",
    "\n",
    "# Define input example\n",
    "input_example = pd.DataFrame({\"prompt\": [\"What is machine learning?\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfd225c8-4e45-483e-9f77-b3e8f4a105ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dee84d2769649b38bfa6ad96515c3cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/06 12:51:00 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n/databricks/python/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09a2c6763e443d5b4ef7fff63122cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/06 12:51:08 INFO mlflow.store.artifact.cloud_artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d15fbc7760945ec9eed8ad1565c8a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /local_disk0/repl_tmp_data/ReplId-74f99-74693-1346d-6/tmp96vi8bay/model/artifacts/model/pytorch_mode…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e7d64d797f4b6bbc7272b1b64bf5c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /local_disk0/repl_tmp_data/ReplId-74f99-74693-1346d-6/tmp96vi8bay/model/artifacts/model/pytorch_mode…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e045f68e62d24befbc3557781b008b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /local_disk0/repl_tmp_data/ReplId-74f99-74693-1346d-6/tmp96vi8bay/model/artifacts/model/model-00002-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126588ce12e4441f83970229f3e0bf50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /local_disk0/repl_tmp_data/ReplId-74f99-74693-1346d-6/tmp96vi8bay/model/artifacts/model/model-00001-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the current base version of torch that is installed, without specific version modifiers\n",
    "# torch_version = torch.__version__.split(\"+\")[0]\n",
    "\n",
    "# Start an MLflow run context and log the MPT-7B model wrapper along with the param-included signature to\n",
    "# allow for overriding parameters at inference time\n",
    "\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "CATALOG =\"capgemini\"\n",
    "SCHEMA = \"chatbot\"\n",
    "registered_model_name = f\"{CATALOG}.{SCHEMA}.llama_pyfunc_model\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"llm_as_pyfunc\") as run:\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"llama_deployment_uc\",\n",
    "        python_model=Llama_pyfunc(),\n",
    "        # NOTE: the artifacts dictionary mapping is critical! This dict is used by the load_context() method in our MPT() class.\n",
    "        artifacts={\"snapshot\": snapshot_location},\n",
    "        pip_requirements=['pandas==1.4.4',\n",
    "    'torch==2.1.0',\n",
    "    'transformers==4.34.0',\n",
    "    'accelerate==0.23.0',\n",
    "    'bitsandbytes==0.41.1',\n",
    "    'tiktoken==0.5.1'],\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b5ad7fa-21db-4e4c-9c81-b648d9b83f37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'capgemini.chatbot.llama_pyfunc_model' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4714a8f47045efb5fe758c09a3cd5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/06 12:51:55 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e00ec80f404824974a2e6a48f839be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading /tmp/tmpxjbnjgpa/llama_deployment_uc/artifacts/model/model-00001-of-00002.safetensors:   0%|      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1594871cba4f31872d3444a70b1e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading /tmp/tmpxjbnjgpa/llama_deployment_uc/artifacts/model/pytorch_model-00001-of-00002.bin:   0%|      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe3f153be744dd2a5b6226302440b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading /tmp/tmpxjbnjgpa/llama_deployment_uc/artifacts/model/pytorch_model-00002-of-00002.bin:   0%|      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7cca541878d4b259769b1a11fe7ddb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading /tmp/tmpxjbnjgpa/llama_deployment_uc/artifacts/model/model-00002-of-00002.safetensors:   0%|      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7583a61d453242e89896398e690f963f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/06 12:52:19 INFO mlflow.store.artifact.cloud_artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a855b81f639747a1bd0b64e6a59b5726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /tmp/tmpxjbnjgpa/llama_deployment_uc/artifacts/model/pytorch_model-00002-of-00002.bin:   0%|        …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f31630e0184272845998586c07eeae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /tmp/tmpxjbnjgpa/llama_deployment_uc/artifacts/model/model-00001-of-00002.safetensors:   0%|        …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48599e3c27ec46b39103475e69d1cbf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /tmp/tmpxjbnjgpa/llama_deployment_uc/artifacts/model/pytorch_model-00001-of-00002.bin:   0%|        …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943f341a1be440fcb66830777eec6713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /tmp/tmpxjbnjgpa/llama_deployment_uc/artifacts/model/model-00002-of-00002.safetensors:   0%|        …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/06 12:53:08 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: capgemini.chatbot.llama_pyfunc_model, version 3\nCreated version '3' of model 'capgemini.chatbot.llama_pyfunc_model'.\n"
     ]
    }
   ],
   "source": [
    "latest_model = mlflow.register_model(f'runs:/{run.info.run_id}/llama_deployment_uc', registered_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5f449c3-cb62-480b-860e-d34aaca0680c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76c4c003-ea29-448e-bf80-7fdd447cde33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a43eeba-9af7-44dc-b163-1b7d225ee58e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.llms import Databricks\n",
    "\n",
    "llm = Databricks(endpoint_name=\"pyfunc_llama_2\",model_kwargs={\"max_tokens\":30})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8906c0b0-f885-4b87-84cd-ad9addf9643c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValidationError\u001B[0m                           Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4261348161710289>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mllm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mwhat is machine learning?\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/langchain/llms/base.py:382\u001B[0m, in \u001B[0;36mBaseLLM.__call__\u001B[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    375\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(prompt, \u001B[38;5;28mstr\u001B[39m):\n",
       "\u001B[1;32m    376\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m    377\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    378\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(prompt)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. If you want to run the LLM on multiple prompts, use \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    379\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`generate` instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    380\u001B[0m     )\n",
       "\u001B[1;32m    381\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (\n",
       "\u001B[0;32m--> 382\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m    383\u001B[0m \u001B[43m        \u001B[49m\u001B[43m[\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    384\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    385\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    386\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtags\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    387\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    388\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    389\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    390\u001B[0m     \u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n",
       "\u001B[1;32m    391\u001B[0m     \u001B[38;5;241m.\u001B[39mtext\n",
       "\u001B[1;32m    392\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/langchain/llms/base.py:234\u001B[0m, in \u001B[0;36mBaseLLM.generate\u001B[0;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    228\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m    229\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    230\u001B[0m         )\n",
       "\u001B[1;32m    231\u001B[0m     run_managers \u001B[38;5;241m=\u001B[39m callback_manager\u001B[38;5;241m.\u001B[39mon_llm_start(\n",
       "\u001B[1;32m    232\u001B[0m         dumpd(\u001B[38;5;28mself\u001B[39m), prompts, invocation_params\u001B[38;5;241m=\u001B[39mparams, options\u001B[38;5;241m=\u001B[39moptions\n",
       "\u001B[1;32m    233\u001B[0m     )\n",
       "\u001B[0;32m--> 234\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_helper\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnew_arg_supported\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n",
       "\u001B[1;32m    236\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    237\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
       "\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_prompts) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/langchain/llms/base.py:178\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    176\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n",
       "\u001B[1;32m    177\u001B[0m         run_manager\u001B[38;5;241m.\u001B[39mon_llm_error(e)\n",
       "\u001B[0;32m--> 178\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[1;32m    179\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mflatten()\n",
       "\u001B[1;32m    180\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m manager, flattened_output \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(run_managers, flattened_outputs):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/langchain/llms/base.py:165\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    155\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_generate_helper\u001B[39m(\n",
       "\u001B[1;32m    156\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[1;32m    157\u001B[0m     prompts: List[\u001B[38;5;28mstr\u001B[39m],\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    161\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n",
       "\u001B[1;32m    162\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n",
       "\u001B[1;32m    163\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    164\u001B[0m         output \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[0;32m--> 165\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m    166\u001B[0m \u001B[43m                \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    167\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    168\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;66;43;03m# TODO: support multiple run managers\u001B[39;49;00m\n",
       "\u001B[1;32m    169\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    170\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    171\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    172\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n",
       "\u001B[1;32m    173\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(prompts, stop\u001B[38;5;241m=\u001B[39mstop)\n",
       "\u001B[1;32m    174\u001B[0m         )\n",
       "\u001B[1;32m    175\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    176\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/langchain/llms/base.py:561\u001B[0m, in \u001B[0;36mLLM._generate\u001B[0;34m(self, prompts, stop, run_manager, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    555\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m prompt \u001B[38;5;129;01min\u001B[39;00m prompts:\n",
       "\u001B[1;32m    556\u001B[0m     text \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[1;32m    557\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(prompt, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    558\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n",
       "\u001B[1;32m    559\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(prompt, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    560\u001B[0m     )\n",
       "\u001B[0;32m--> 561\u001B[0m     generations\u001B[38;5;241m.\u001B[39mappend([\u001B[43mGeneration\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m])\n",
       "\u001B[1;32m    562\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m LLMResult(generations\u001B[38;5;241m=\u001B[39mgenerations)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/langchain/load/serializable.py:74\u001B[0m, in \u001B[0;36mSerializable.__init__\u001B[0;34m(self, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m---> 74\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     75\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lc_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pydantic/main.py:341\u001B[0m, in \u001B[0;36mpydantic.main.BaseModel.__init__\u001B[0;34m()\u001B[0m\n",
       "\n",
       "\u001B[0;31mValidationError\u001B[0m: 1 validation error for Generation\n",
       "text\n",
       "  str type expected (type=type_error.str)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValidationError\u001B[0m                           Traceback (most recent call last)\nFile \u001B[0;32m<command-4261348161710289>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mllm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mwhat is machine learning?\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/langchain/llms/base.py:382\u001B[0m, in \u001B[0;36mBaseLLM.__call__\u001B[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001B[0m\n\u001B[1;32m    375\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(prompt, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    376\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    377\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    378\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(prompt)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. If you want to run the LLM on multiple prompts, use \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    379\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`generate` instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    380\u001B[0m     )\n\u001B[1;32m    381\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m--> 382\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    383\u001B[0m \u001B[43m        \u001B[49m\u001B[43m[\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    384\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    385\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    386\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtags\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    387\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    388\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    389\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    390\u001B[0m     \u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    391\u001B[0m     \u001B[38;5;241m.\u001B[39mtext\n\u001B[1;32m    392\u001B[0m )\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/langchain/llms/base.py:234\u001B[0m, in \u001B[0;36mBaseLLM.generate\u001B[0;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001B[0m\n\u001B[1;32m    228\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    229\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    230\u001B[0m         )\n\u001B[1;32m    231\u001B[0m     run_managers \u001B[38;5;241m=\u001B[39m callback_manager\u001B[38;5;241m.\u001B[39mon_llm_start(\n\u001B[1;32m    232\u001B[0m         dumpd(\u001B[38;5;28mself\u001B[39m), prompts, invocation_params\u001B[38;5;241m=\u001B[39mparams, options\u001B[38;5;241m=\u001B[39moptions\n\u001B[1;32m    233\u001B[0m     )\n\u001B[0;32m--> 234\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_helper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    235\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnew_arg_supported\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    236\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    237\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_prompts) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/langchain/llms/base.py:178\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[1;32m    176\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n\u001B[1;32m    177\u001B[0m         run_manager\u001B[38;5;241m.\u001B[39mon_llm_error(e)\n\u001B[0;32m--> 178\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    179\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[1;32m    180\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m manager, flattened_output \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(run_managers, flattened_outputs):\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/langchain/llms/base.py:165\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_generate_helper\u001B[39m(\n\u001B[1;32m    156\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    157\u001B[0m     prompts: List[\u001B[38;5;28mstr\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    161\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    162\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    164\u001B[0m         output \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 165\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    166\u001B[0m \u001B[43m                \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    167\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    168\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;66;43;03m# TODO: support multiple run managers\u001B[39;49;00m\n\u001B[1;32m    169\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    171\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    172\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    173\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(prompts, stop\u001B[38;5;241m=\u001B[39mstop)\n\u001B[1;32m    174\u001B[0m         )\n\u001B[1;32m    175\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    176\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/langchain/llms/base.py:561\u001B[0m, in \u001B[0;36mLLM._generate\u001B[0;34m(self, prompts, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    555\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m prompt \u001B[38;5;129;01min\u001B[39;00m prompts:\n\u001B[1;32m    556\u001B[0m     text \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    557\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(prompt, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    558\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    559\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(prompt, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    560\u001B[0m     )\n\u001B[0;32m--> 561\u001B[0m     generations\u001B[38;5;241m.\u001B[39mappend([\u001B[43mGeneration\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m])\n\u001B[1;32m    562\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m LLMResult(generations\u001B[38;5;241m=\u001B[39mgenerations)\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/langchain/load/serializable.py:74\u001B[0m, in \u001B[0;36mSerializable.__init__\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 74\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lc_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pydantic/main.py:341\u001B[0m, in \u001B[0;36mpydantic.main.BaseModel.__init__\u001B[0;34m()\u001B[0m\n\n\u001B[0;31mValidationError\u001B[0m: 1 validation error for Generation\ntext\n  str type expected (type=type_error.str)",
       "errorSummary": "<span class='ansi-red-fg'>ValidationError</span>: 1 validation error for Generation\ntext\n  str type expected (type=type_error.str)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm(prompt=\"what is machine learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b2a5e88-6ac4-4d18-a0b4-6b09947757b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Llama_PyFunc_First_Link1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
